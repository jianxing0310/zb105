{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 單天版 -- 將網頁寫入txt裡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "with open('ltn.txt', 'w') as f:\n",
    "    url = 'http://news.ltn.com.tw/newspaper/focus/20160310'\n",
    "    res = requests.get(url)\n",
    "    soup = bs(res.text)\n",
    "\n",
    "    Times = 'http://news.ltn.com.tw'\n",
    "    sort = soup.select('.newspaper a')\n",
    "    for a in xrange(0, 12):\n",
    "        url2 = Times + sort[a]['href'] #分類網址\n",
    "        res2 = requests.get(url2)\n",
    "        soup2 = bs(res2.text)\n",
    "        print len(soup2.select('.p_num'))\n",
    "        if len(soup2.select('.p_num')) == 1 : #判斷此類別新聞是否一頁\n",
    "            title = soup2.select('.picword')\n",
    "            title_num = len(soup2.select('.picword')) #計算有幾篇文章(每天新聞數量不同)\n",
    "            for i in xrange(0 , title_num):   \n",
    "                url1=Times+title[i]['href']   #文章網址\n",
    "                f.write(url1  + '\\n')  #寫入text\n",
    "        else:\n",
    "            for page1 in xrange (1,len(soup2.select('.p_num'))+1): #此類別新聞超過一頁時執行\n",
    "                url3= url2+'?page={}'.format(page1)\n",
    "                print url3\n",
    "                res3 = requests.get(url3)\n",
    "                soup3 = bs(res3.text)\n",
    "                title = soup3.select('.picword')\n",
    "                title_num1 = len(soup3.select('.picword')) #計算有幾篇文章(每天新聞數量不同)\n",
    "                for i in xrange(0 , title_num1):   \n",
    "                    url1=Times+title[i]['href']   #文章網址\n",
    "                    f.write(url1  + '\\n')  #寫入text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 單天版-- 讀取txt裡網頁 並寫成檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8e2bd564853b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;34m'opinion'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mwr_talk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m             \u001b[1;31m#print x[0],x[1],x[2]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-8e2bd564853b>\u001b[0m in \u001b[0;36mwr_talk\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdiv3\u001b[0m \u001b[1;32min\u001b[0m \u001b[0messay\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0messay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mpage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdiv3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m       \u001b[1;31m#加入內文\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtital\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import sys\n",
    "\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding(\"utf-8\")   #解決UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-6:ordinal not in range(128) \n",
    "\n",
    "def wr_all(url):  #方法 大部分的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('#newstext p , h4')\n",
    "    tital = soup.select('h1 ')\n",
    "    date =  soup.select('#newstext span ')\n",
    "    category = soup.select('.guide  a ')\n",
    "    keyword= soup.select('.con_keyword')\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文       \n",
    "    return tital[0].text,date[0].text,category[1].text, page\n",
    "        \n",
    "def wr_sport(url):  #方法 運動類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)    \n",
    "    essay = soup.select('h4 , p')\n",
    "    tital = soup.select('.Btitle ')\n",
    "    date =  soup.select('.news_content .date ')\n",
    "    category = \"體育\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文\n",
    "        time.sleep(1)     \n",
    "    return tital[0].text,date[0].text,category, page\n",
    "     \n",
    "def wr_ent(url):  #方法 娛樂類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('p')\n",
    "    tital = soup.select('.news_content h1 ')\n",
    "    date =  soup.select('.news_content .date')\n",
    "    category = \"娛樂\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文\n",
    "        time.sleep(1)    \n",
    "    return tital[0].text,date[0].text,category, page\n",
    "\n",
    "def wr_local(url): #方法 地方類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('#newstext p , h4')\n",
    "    tital = soup.select('h1 ')\n",
    "    date =  soup.select('#newstext span ')\n",
    "    category1 = \"地方\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文\n",
    "        time.sleep(1)\n",
    "    return tital[0].text,date[0].text,category1, page\n",
    "    \n",
    "def wr_talk(url): # 方法 言論類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('.cont p')\n",
    "    tital = soup.select('h2 ')\n",
    "    date =  soup.select('.writer span')\n",
    "    category = \"言論\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文\n",
    "        time.sleep(1)    \n",
    "    return tital[0].text,date[0].text,category, page\n",
    "        \n",
    "    \n",
    "        \n",
    "with open('ltn.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        url = line.strip()\n",
    "        m = re.match('.*news/(\\w+)/paper/(\\w+)' ,url)  #正規表達法 切出m.group(1)類別 與 m.group(2)網頁最後的數字(用來當檔名)\n",
    "        print m.group(1)\n",
    "        if m.group(1) == 'sports': #判斷是否為體育類\n",
    "            x=  wr_sport(url) #使用方法\n",
    "            #print x[0],x[1],x[2]\n",
    "            name =  m.group(2)\n",
    "            f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "            iii_num = len(x)\n",
    "            for iii in xrange (0, iii_num):\n",
    "                f.write( x[iii] + '\\n') #寫入內文      \n",
    "            f.close()\n",
    "            time.sleep(1)\n",
    "            \n",
    "        elif m.group(1) == 'entertainment': #判斷是否為娛樂類\n",
    "            x= wr_ent(url) #使用方法\n",
    "            #print x[0],x[1],x[2]\n",
    "            name =  m.group(2)\n",
    "            f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "            iii_num = len(x) #計算陣列長度\n",
    "            for iii in xrange (0, iii_num):\n",
    "                f.write( x[iii] + '\\n') #寫入內文           \n",
    "            f.close()\n",
    "            time.sleep(1)\n",
    "            \n",
    "        elif m.group(1) =='local': #判斷是否為地方類\n",
    "            x= wr_local(url) #使用方法\n",
    "            #print x[0],x[1],x[2]\n",
    "            name =  m.group(2)\n",
    "            f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "            iii_num = len(x)  \n",
    "            for iii in xrange (0, iii_num):\n",
    "                f.write( x[iii] + '\\n') #寫入內文         \n",
    "            f.close()\n",
    "            time.sleep(1)\n",
    "            \n",
    "        elif m.group(1) =='opinion': #判斷是否為言論類\n",
    "            x= wr_talk(url) #使用方法\n",
    "            #print x[0],x[1],x[2]\n",
    "            name =  m.group(2)\n",
    "            f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "            iii_num = len(x)\n",
    "            for iii in xrange (0, iii_num):\n",
    "                f.write( x[iii] + '\\n') #寫入內文          \n",
    "            f.close()\n",
    "            time.sleep(1)\n",
    "            \n",
    "        else:\n",
    "            x= wr_all(url) #使用方法\n",
    "            #print x[0],x[1],x[2]\n",
    "            name =  m.group(2)\n",
    "            f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "            iii_num = len(x)\n",
    "            for iii in xrange (0, iii_num):\n",
    "                f.write( x[iii] + '\\n') #寫入內文        \n",
    "            f.close()\n",
    "            time.sleep(1)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全年無休版--將網頁寫入txt裡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全年無休版--讀取txt裡網頁 並寫成檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/BIG DATA/Desktop/python/essay/20160312.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ab1b4342da1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0mdatestring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstartdate\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdaynumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%Y%m%d\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/BIG DATA/Desktop/python/essay/{}.txt'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/BIG DATA/Desktop/python/essay/20160312.txt'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import sys\n",
    "import datetime as dt\n",
    "\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding(\"utf-8\")   #解決UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-6:ordinal not in range(128) \n",
    "\n",
    "\n",
    "\n",
    "def wr_all(url):  #方法 大部分的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('#newstext p , h4')\n",
    "    tital = soup.select('h1 ')\n",
    "    date =  soup.select('#newstext span ')\n",
    "    category = soup.select('.guide  a ')\n",
    "    keyword= soup.select('.con_keyword')\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文       \n",
    "    return tital[0].text,date[0].text,category[1].text, page\n",
    "        \n",
    "def wr_sport(url):  #方法 運動類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)    \n",
    "    essay = soup.select('h4 , p')\n",
    "    tital = soup.select('.Btitle ')\n",
    "    date =  soup.select('.news_content .date ')\n",
    "    category = \"體育\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文\n",
    "        time.sleep(1)     \n",
    "    return tital[0].text,date[0].text,category, page\n",
    "     \n",
    "def wr_ent(url):  #方法 娛樂類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('p')\n",
    "    tital = soup.select('.news_content h1 ')\n",
    "    date =  soup.select('.news_content .date')\n",
    "    category = \"娛樂\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文\n",
    "        time.sleep(1)    \n",
    "    return tital[0].text,date[0].text,category, page\n",
    "\n",
    "def wr_local(url): #方法 地方類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('#newstext p , h4')\n",
    "    tital = soup.select('h1 ')\n",
    "    date =  soup.select('#newstext span ')\n",
    "    category1 = \"地方\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文\n",
    "        time.sleep(1)\n",
    "    return tital[0].text,date[0].text,category1, page\n",
    "    \n",
    "def wr_talk(url): # 方法 言論類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('.cont p')\n",
    "    tital = soup.select('h2 ')\n",
    "    date =  soup.select('.writer span')\n",
    "    category = \"言論\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文\n",
    "        time.sleep(1)    \n",
    "    return tital[0].text,date[0].text,category, page\n",
    "        \n",
    "    \n",
    "startdate = dt.datetime(2016, 3,9)  #輸入開始日期\n",
    "endate = dt.datetime(2016, 3,12)       #輸入結束日期\n",
    "totaldays = (endate - startdate).days + 1  #計算有幾天\n",
    "for daynumber in range(totaldays):\n",
    "    datestring = (startdate + dt.timedelta(days = daynumber)).date().strftime(\"%Y%m%d\")  \n",
    "        \n",
    "    with open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(datestring), 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            url = line.strip()\n",
    "            m = re.match('.*news/(\\w+)/paper/(\\w+)' ,url)  #正規表達法 切出m.group(1)類別 與 m.group(2)網頁最後的數字(用來當檔名)\n",
    "            print m.group(1)\n",
    "            if m.group(1) == 'sports': #判斷是否為體育類\n",
    "                x=  wr_sport(url) #使用方法\n",
    "                #print x[0],x[1],x[2]\n",
    "                name =  m.group(2)\n",
    "                f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "                iii_num = len(x)\n",
    "                for iii in xrange (0, iii_num):\n",
    "                    f.write( x[iii] + '\\n') #寫入內文      \n",
    "                f.close()\n",
    "                time.sleep(1)\n",
    "\n",
    "            elif m.group(1) == 'entertainment': #判斷是否為娛樂類\n",
    "                x= wr_ent(url) #使用方法\n",
    "                #print x[0],x[1],x[2]\n",
    "                name =  m.group(2)\n",
    "                f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "                iii_num = len(x) #計算陣列長度\n",
    "                for iii in xrange (0, iii_num):\n",
    "                    f.write( x[iii] + '\\n') #寫入內文           \n",
    "                f.close()\n",
    "                time.sleep(1)\n",
    "\n",
    "            elif m.group(1) =='local': #判斷是否為地方類\n",
    "                x= wr_local(url) #使用方法\n",
    "                #print x[0],x[1],x[2]\n",
    "                name =  m.group(2)\n",
    "                f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "                iii_num = len(x)  \n",
    "                for iii in xrange (0, iii_num):\n",
    "                    f.write( x[iii] + '\\n') #寫入內文         \n",
    "                f.close()\n",
    "                time.sleep(1)\n",
    "\n",
    "            elif m.group(1) =='opinion': #判斷是否為言論類\n",
    "                x= wr_talk(url) #使用方法\n",
    "                #print x[0],x[1],x[2]\n",
    "                name =  m.group(2)\n",
    "                f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "                iii_num = len(x)\n",
    "                for iii in xrange (0, iii_num):\n",
    "                    f.write( x[iii] + '\\n') #寫入內文          \n",
    "                f.close()\n",
    "                time.sleep(1)\n",
    "\n",
    "            else:\n",
    "                x= wr_all(url) #使用方法\n",
    "                #print x[0],x[1],x[2]\n",
    "                name =  m.group(2)\n",
    "                f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "                iii_num = len(x)\n",
    "                for iii in xrange (0, iii_num):\n",
    "                    f.write( x[iii] + '\\n') #寫入內文        \n",
    "                f.close()\n",
    "                time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 寫進單一檔案  new 3/14(修正版)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/BIG DATA/Desktop/python/essay/20160310.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a22eded005d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0mdatestring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstartdate\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdaynumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%Y%m%d\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/BIG DATA/Desktop/python/essay/{}.txt'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/BIG DATA/Desktop/python/essay/ltn.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/BIG DATA/Desktop/python/essay/20160310.txt'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import sys\n",
    "import datetime as dt\n",
    "\n",
    "reload(sys)  \n",
    "sys.setdefaultencoding(\"utf-8\")   #解決UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-6:ordinal not in range(128) \n",
    "\n",
    "\n",
    "\n",
    "def wr_all(url):  #方法 大部分的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('#newstext p , h4')\n",
    "    tital = soup.select('h1 ')\n",
    "    date =  soup.select('#newstext span ')\n",
    "    category = soup.select('.guide  a ')\n",
    "    keyword= soup.select('.con_keyword')\n",
    "    photo = soup.select('#newsphoto p')\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    if len(photo)==0 :\n",
    "        for div3 in essay[0:(len(essay))]:\n",
    "            page += div3.text       #加入內文\n",
    "    else:\n",
    "        for div3 in essay[0:(len(essay)-len(photo))]:\n",
    "            page += div3.text\n",
    "    return tital[0].text,date[0].text,category[1].text, page\n",
    "        \n",
    "def wr_sport(url):  #方法 運動類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)    \n",
    "    essay = soup.select('h4 , p')\n",
    "    tital = soup.select('.Btitle ')\n",
    "    date =  soup.select('.news_content .date ')\n",
    "    category = \"體育\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[1:(len(essay))]:\n",
    "        page += div3.text       #加入內文     \n",
    "    return tital[0].text,date[0].text,category, page\n",
    "     \n",
    "def wr_ent(url):  #方法 娛樂類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('p')\n",
    "    tital = soup.select('.news_content h1 ')\n",
    "    date =  soup.select('.news_content .date')\n",
    "    category = \"娛樂\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[1:(len(essay))]:\n",
    "        page += div3.text       #加入內文    \n",
    "    return tital[0].text,date[0].text,category, page\n",
    "\n",
    "def wr_local(url): #方法 地方類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('#newstext p , h4')\n",
    "    tital = soup.select('h1 ')\n",
    "    date =  soup.select('#newstext span ')\n",
    "    photo = soup.select('#newsphoto p')\n",
    "    category1 = \"地方\"\n",
    "    page=\"\"#給一個字串,存放內文用 \n",
    "    if len(photo)==0 :\n",
    "        for div3 in essay[0:(len(essay))]:\n",
    "            page += div3.text       #加入內文\n",
    "    else:\n",
    "        for div3 in essay[0:(len(essay)-len(photo)-len(photo))]:\n",
    "            page += div3.text\n",
    "    return tital[0].text,date[0].text,category1, page\n",
    "    \n",
    "def wr_talk(url): # 方法 言論類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('.cont p')\n",
    "    tital = soup.select('h2 ')\n",
    "    date =  soup.select('.writer span')\n",
    "    category = \"言論\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文   \n",
    "    return tital[0].text,date[0].text,category, page\n",
    "        \n",
    "    \n",
    "startdate = dt.datetime(2016, 3,9)  #輸入開始日期\n",
    "endate = dt.datetime(2016, 3,12)       #輸入結束日期\n",
    "totaldays = (endate - startdate).days + 1  #計算有幾天\n",
    "for daynumber in range(totaldays):\n",
    "    datestring = (startdate + dt.timedelta(days = daynumber)).date().strftime(\"%Y%m%d\")  \n",
    "    \n",
    "    with open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(datestring), 'r') as f:\n",
    "        a = open('C:/Users/BIG DATA/Desktop/python/essay/ltn.txt','w')\n",
    "        for line in f.readlines():\n",
    "            url = line.strip()\n",
    "            m = re.match('.*news/(\\w+)/paper/(\\w+)' ,url)  #正規表達法 切出m.group(1)類別 與 m.group(2)網頁最後的數字(用來當檔名)\n",
    "            # print m.group(1)\n",
    "            if m.group(1) == 'sports': #判斷是否為體育類\n",
    "                x=  wr_sport(url) #使用方法\n",
    "                #print x[0],x[1],x[2]\n",
    "                a.write( x[0] + '#'+x[1] + '#'+ x[2] + '#'+ x[3]+'\\n' ) #寫入內文      \n",
    "                time.sleep(0.5)\n",
    "\n",
    "            elif m.group(1) == 'entertainment': #判斷是否為娛樂類\n",
    "                x= wr_ent(url) #使用方法\n",
    "                #print x[0],x[1],x[2]\n",
    "                a.write( x[0] + '#'+x[1] + '#'+ x[2] + '#'+ x[3]+'\\n' ) #寫入內文      \n",
    "                time.sleep(0.5)\n",
    "\n",
    "            elif m.group(1) =='local': #判斷是否為地方類\n",
    "                x= wr_local(url) #使用方法\n",
    "                #print x[0],x[1],x[2]\n",
    "                a.write( x[0] + '#'+x[1] + '#'+ x[2] + '#'+ x[3]+'\\n'  ) #寫入內文      \n",
    "                time.sleep(0.5)\n",
    "\n",
    "            elif m.group(1) =='opinion': #判斷是否為言論類\n",
    "                x= wr_talk(url) #使用方法\n",
    "                #print x[0],x[1],x[2]\n",
    "                a.write( x[0] + '#'+x[1] + '#'+ x[2] + '#'+ x[3]+'\\n'  ) #寫入內文      \n",
    "                time.sleep(0.5)\n",
    "\n",
    "            else:\n",
    "                x= wr_all(url) #使用方法\n",
    "                #print x[0],x[1],x[2]\n",
    "                a.write( x[0] + '#'+x[1] + '#'+ x[2] + '#'+ x[3]+'\\n'  ) #寫入內文      \n",
    "                time.sleep(0.5)\n",
    "        a.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "with open('ltn.txt', 'w') as f:\n",
    "    url = 'http://news.ltn.com.tw/newspaper/focus/20160310'\n",
    "    res = requests.get(url)\n",
    "    soup = bs(res.text)\n",
    "\n",
    "    Times = 'http://news.ltn.com.tw'\n",
    "    sort = soup.select('.newspaper a')\n",
    "    for a in xrange(0, 12):\n",
    "        url2 = Times + sort[a]['href'] #分類網址\n",
    "        res2 = requests.get(url2)\n",
    "        soup2 = bs(res2.text)\n",
    "        print len(soup2.select('.p_num'))\n",
    "        if len(soup2.select('.p_num')) == 1 : #判斷此類別新聞是否一頁\n",
    "            title = soup2.select('.picword')\n",
    "            title_num = len(soup2.select('.picword')) #計算有幾篇文章(每天新聞數量不同)\n",
    "            for i in xrange(0 , title_num):   \n",
    "                url1=Times+title[i]['href']   #文章網址\n",
    "                f.write(url1  + '\\n')  #寫入text\n",
    "        else:\n",
    "            for page1 in xrange (1,len(soup2.select('.p_num'))+1): #此類別新聞超過一頁時執行\n",
    "                url3= url2+'?page={}'.format(page1)\n",
    "                print url3\n",
    "                res3 = requests.get(url3)\n",
    "                soup3 = bs(res3.text)\n",
    "                title = soup3.select('.picword')\n",
    "                title_num1 = len(soup3.select('.picword')) #計算有幾篇文章(每天新聞數量不同)\n",
    "                for i in xrange(0 , title_num1):   \n",
    "                    url1=Times+title[i]['href']   #文章網址\n",
    "                    f.write(url1  + '\\n')  #寫入text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 標題,類別,內文,關鍵詞,新聞連結網址,日期(格式yyyyMMdd),點擊次數\n",
    "\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import sys\n",
    "import json\n",
    "import ast  #轉換成json需要套件\n",
    "\n",
    "\n",
    "#reload(sys)  \n",
    "#sys.setdefaultencoding(\"utf-8\")   #解決UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-6:ordinal not in range(128) \n",
    "\n",
    "def wr_all(url):  #方法 大部分的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('#newstext p , h4')\n",
    "    tital = soup.select('h1 ')\n",
    "    date =  soup.select('#newstext span ')\n",
    "    category = soup.select('.guide  a ')\n",
    "    keyword= soup.select('.con_keyword')\n",
    "    #page=\"\"                          #給一個字串,存放內文用           \n",
    "    #for div3 in essay[0:(len(essay))]:\n",
    "        #page += div3.text       #加入內文       \n",
    "    return tital[0].text,date[0].text,category[1].text, #page\n",
    "        \n",
    "def wr_sport(url):  #方法 運動類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)    \n",
    "    essay = soup.select('h4 , p')\n",
    "    tital = soup.select('.Btitle ')\n",
    "    date =  soup.select('.news_content .date ')\n",
    "    category = \"體育\"\n",
    "    #page=\"\"                          #給一個字串,存放內文用           \n",
    "    #for div3 in essay[0:(len(essay))]:\n",
    "        #page += div3.text       #加入內文\n",
    "        #time.sleep(1)     \n",
    "    return tital[0].text,date[0].text,category, #page\n",
    "     \n",
    "def wr_ent(url):  #方法 娛樂類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('p')\n",
    "    tital = soup.select('.news_content h1 ')\n",
    "    date =  soup.select('.news_content .date')\n",
    "    category = \"娛樂\"\n",
    "    #page=\"\"                          #給一個字串,存放內文用           \n",
    "    #for div3 in essay[0:(len(essay))]:\n",
    "        #page += div3.text       #加入內文\n",
    "        #time.sleep(1)    \n",
    "    return tital[0].text,date[0].text,category, #page\n",
    "\n",
    "def wr_local(url): #方法 地方類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('#newstext p , h4')\n",
    "    tital = soup.select('h1 ')\n",
    "    date =  soup.select('#newstext span ')\n",
    "    category1 = \"地方\"\n",
    "    #page=\"\"                          #給一個字串,存放內文用           \n",
    "    #for div3 in essay[0:(len(essay))]:\n",
    "        #page += div3.text       #加入內文\n",
    "        #time.sleep(1)\n",
    "    return tital[0].text,date[0].text,category1, #page\n",
    "    \n",
    "def wr_talk(url): # 方法 言論類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('.cont p')\n",
    "    tital = soup.select('h2 ')\n",
    "    date =  soup.select('.writer span')\n",
    "    category = \"言論\"\n",
    "    #page=\"\"                          #給一個字串,存放內文用           \n",
    "    #for div3 in essay[0:(len(essay))]:\n",
    "        #page += div3.text       #加入內文\n",
    "        #time.sleep(1)    \n",
    "    return tital[0].text,date[0].text,category, #page\n",
    "        \n",
    "    \n",
    "        \n",
    "with open('ltn.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        url = line.strip()\n",
    "        m = re.match('.*news/(\\w+)/paper/(\\w+)' ,url)  #正規表達法 切出m.group(1)類別 與 m.group(2)網頁最後的數字(用來當檔名)\n",
    "        print m.group(1)\n",
    "        if m.group(1) == 'sports': #判斷是否為體育類\n",
    "            x=  wr_sport(url) #使用方法\n",
    "            print x[0],x[1],x[2]\n",
    "            #name =  m.group(2)\n",
    "            #f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "            #iii_num = len(x)\n",
    "            #for iii in xrange (0, iii_num):\n",
    "                #f.write( x[iii] + '\\n') #寫入內文      \n",
    "            #f.close()\n",
    "            #time.sleep(1)\n",
    "            \n",
    "        elif m.group(1) == 'entertainment': #判斷是否為娛樂類\n",
    "            x= wr_ent(url) #使用方法\n",
    "            print x[0],x[1],x[2]\n",
    "            #name =  m.group(2)\n",
    "            #f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "            #iii_num = len(x) #計算陣列長度\n",
    "            #for iii in xrange (0, iii_num):\n",
    "              #  f.write( x[iii] + '\\n') #寫入內文           \n",
    "            #f.close()\n",
    "            #time.sleep(1)\n",
    "            \n",
    "        elif m.group(1) =='local': #判斷是否為地方類\n",
    "            x= wr_local(url) #使用方法\n",
    "            print x[0],x[1],x[2]\n",
    "            #name =  m.group(2)\n",
    "            #f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "            #iii_num = len(x)  \n",
    "            #for iii in xrange (0, iii_num):\n",
    "                #f.write( x[iii] + '\\n') #寫入內文         \n",
    "            #f.close()\n",
    "            #time.sleep(1)\n",
    "            \n",
    "        elif m.group(1) =='opinion': #判斷是否為言論類\n",
    "            x= wr_talk(url) #使用方法\n",
    "            print x[0],x[1],x[2]\n",
    "            #name =  m.group(2)\n",
    "            #f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "            #iii_num = len(x)\n",
    "            #for iii in xrange (0, iii_num):\n",
    "                #f.write( x[iii] + '\\n') #寫入內文          \n",
    "            #f.close()\n",
    "            #time.sleep(1)\n",
    "            \n",
    "        else:\n",
    "            x= wr_all(url) #使用方法\n",
    "            print x[0],x[1],x[2]\n",
    "            #name =  m.group(2)\n",
    "            #f = open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(name),'w')\n",
    "            #iii_num = len(x)\n",
    "            #for iii in xrange (0, iii_num):\n",
    "                #f.write( x[iii] + '\\n') #寫入內文        \n",
    "            #f.close()\n",
    "            #time.sleep(1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "醫院評鑑如同高裝檢 段宜康：哪有不造假的？ 2016-03-20  21:19 政治 〔即時新聞／綜合報導〕衛生福利部每年都會委託醫策會，對全台各醫院進行評鑑，引來廣大醫護人員不滿，民進黨立委段宜康對此表示，醫院評鑑如同高裝檢，而「高裝檢哪有不造假的？」段宜康在臉書上轉貼醫師洪浩雲的PO文指出，「衛福部大概沒有人當過兵」，所有當過兵的民眾心裡都明白「高裝檢哪有不造假的？」，而醫院的評鑑制度就如同軍中的高裝檢，衛福部每年委託醫策會到醫院打分數，「先通知要準備哪些資料、看哪些設備」。段宜康批評，若是米其林評鑑也使用如同高裝檢般「官樣文章」，「一定成為笑柄」，衛福部對於我國的醫院評鑑，「不學舉世信服的米其林；去學了自欺欺人的高裝檢」；醫師洪浩雲亦認為，政府官員如果覺得醫院評鑑相當重要，應重視現實層面的問題。段宜康臉書全文衞福部大概沒有人當過兵。當過兵的人都知道：高裝檢哪有不造假的？衛福部委託醫策會，每年固定時間、排定時程來檢查醫院、打分數。先通知要準備哪些資料、看哪些設備。各位後備軍人和現役阿兵哥：是不是很熟悉？醫院評鑑就是高裝檢！米...民進黨立委段宜康表示，醫院評鑑如同高裝檢，而「高裝檢哪有不造假的？」。（資料照，記者羅沛德攝）\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "\n",
    "url  = 'http://news.ltn.com.tw/news/politics/breakingnews/1638642'\n",
    "\n",
    "res = requests.get(url)\n",
    "res.encoding = \"UTF-8\"\n",
    "soup = bs(res.text)\n",
    "essay = soup.select('#newstext p , h4')\n",
    "tital = soup.select('h1 ')\n",
    "date =  soup.select('#newstext span ')\n",
    "category = soup.select('.guide  a ')\n",
    "keyword= soup.select('.con_keyword')\n",
    "page=\"\"                          #給一個字串,存放內文用           \n",
    "for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text       #加入內文       \n",
    "print tital[0].text,date[0].text,category[1].text, page\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
