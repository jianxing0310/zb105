{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from E:\\text_mining\\dict.txt ...\n",
      "DEBUG:jieba:Building prefix dict from E:\\text_mining\\dict.txt ...\n",
      "Loading model from cache c:\\users\\bigdat~1\\appdata\\local\\temp\\jieba.u144803eaa96482ea338b54ad8c7f9634.cache\n",
      "DEBUG:jieba:Loading model from cache c:\\users\\bigdat~1\\appdata\\local\\temp\\jieba.u144803eaa96482ea338b54ad8c7f9634.cache\n",
      "Loading model cost 0.896 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.896 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pymongo import MongoClient \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import re\n",
    "import jieba\n",
    "jieba.set_dictionary('E:/text_mining/dict.txt')  #切換至中文繁體字庫\n",
    "jieba.load_userdict('E:/text_mining/dict_keyw.txt')  #\n",
    "jieba.load_userdict('E:/text_mining/dict_cbdic.txt')  #\n",
    "#預設就是自己\n",
    "client = MongoClient('10.120.28.14',27017)  #不用修改  字典檔請改位子\n",
    "#client = MongoClient('localhost:27017')  #不用修改  字典檔請改位子\n",
    "\n",
    "database = client['news']\n",
    "collection =database['news']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 月開始\n",
      "10864\n",
      "13155\n",
      "13180\n",
      "1866\n",
      "1 月,共有 39065 篇\n",
      "2 月開始\n",
      "10289\n",
      "10384\n",
      "11712\n",
      "2016023 count is 0\n",
      "2 月,共有 32385 篇\n",
      "3 月開始\n",
      "12529\n",
      "13157\n",
      "12733\n",
      "3073\n",
      "3 月,共有 41492 篇\n"
     ]
    }
   ],
   "source": [
    "for aa in range(1,4):\n",
    "    print aa,\"月開始\"\n",
    "    e=0\n",
    "    for ii in range(0,4):        \n",
    "        a=collection.find(\n",
    "                {\"$and\":[                   \n",
    "                        {\"comp\":{\"$regex\":\"udn\"}},\n",
    "                        {\"date\":{\"$regex\":\"20160{}{}\".format(aa,ii)} }          #計算當月文章數\n",
    "                        ]},{\"_id\":0}).count()  \n",
    "        if a==0:\n",
    "            print \"---------------------------------------\"\n",
    "            print \"20160{}{}\".format(aa,ii),\"count is 0\"\n",
    "            print \"---------------------------------------\"\n",
    "\n",
    "        else:\n",
    "            e=e+a\n",
    "            print a\n",
    "    print aa,\"月,共有\",e,\"篇\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "a=collection.find(\n",
    "                {\"$and\":[                   \n",
    "                        {\"comp\":{\"$regex\":\"udn\"}},\n",
    "                        {\"date\":{\"$regex\":\"2016023\" }}        #計算當月文章數\n",
    "                        ]},{\"_id\":0}).count()\n",
    "\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-72589fac15d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# titile 放文本\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# 拿到所有的關鍵詞\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\BIG DATA\\Anaconda2\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1303\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m         \"\"\"\n\u001b[1;32m-> 1305\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1306\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\BIG DATA\\Anaconda2\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 817\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\BIG DATA\\Anaconda2\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    762\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[0;32m    765\u001b[0m                                  \" contain stop words\")\n\u001b[0;32m    766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    " content =[]    \n",
    "for post in collection.find(\n",
    "    {\"$and\":[                   \n",
    "            {\"comp\":{\"$regex\":\"udn\"}},                              #尋找一個月新聞(報社手動修改)\n",
    "            {\"date\":{\"$regex\":\"2016023\"}}        #10-12月請將0去掉\n",
    "            ]}): \n",
    "    summary = post['content']\n",
    "    content.append('/'.join(jieba.cut(summary)))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(content)  # titile 放文本\n",
    "weight = X.toarray()\n",
    "features = vectorizer.get_feature_names()     # 拿到所有的關鍵詞  \n",
    "\n",
    "top_features = []\n",
    "for n in range(0,a):  #迴圈參考上面的總文章數\n",
    "    indices = np.argsort(weight[n])[::-1]  # transformer = TfidfTransformer()  #X.toarray()[5] 是第幾篇新聞的意思\n",
    "    # 看TOP多少的詞\n",
    "    top_n = 40\n",
    "    top_features.append([features[i] for i in indices[:top_n]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nono\n"
     ]
    }
   ],
   "source": [
    "a=collection.find(\n",
    "                {\"$and\":[                   \n",
    "                        {\"comp\":{\"$regex\":\"udn\"}},\n",
    "                        {\"date\":{\"$regex\":\"2016023\" }}        #計算當月文章數\n",
    "                        ]},{\"_id\":0}).count()\n",
    "if a<0:\n",
    "    print a\n",
    "else:\n",
    "    print \"nono\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import time\n",
    "for aa in range(1,4):\n",
    "    for ii in range (1,4):   # 1-9月    10月後改為10-13再執行\n",
    "        a=collection.find(\n",
    "            {\"$and\":[                   \n",
    "                    {\"comp\":{\"$regex\":\"udn\"}},\n",
    "                    {\"date\":{:\"20160{}{}\".format(aa,ii)}}           #計算當月文章數\n",
    "                    ]},{\"_id\":0}).count()    \n",
    "        content =[]\n",
    "        tStart=time.time() #計算所需時間(計時開始)\n",
    "        if a ==0:\n",
    "            print \"20160{}{}\".format(aa,ii)\n",
    "        else:\n",
    "            for post in collection.find(\n",
    "                {\"$and\":[                   \n",
    "                        {\"comp\":{\"$regex\":\"udn\"}},\n",
    "                        {\"date\":{:\"20160{}{}\".format(aa,ii)}}           #計算當月文章數\n",
    "                        ]},{\"_id\":0})\n",
    "                summary = post['content']\n",
    "                content.append('/'.join(jieba.cut(summary)))\n",
    "\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            X = vectorizer.fit_transform(content)  # titile 放文本\n",
    "            weight = X.toarray()\n",
    "            features = vectorizer.get_feature_names()     # 拿到所有的關鍵詞  \n",
    "\n",
    "            top_features = []\n",
    "            for n in range(0,a):  #迴圈參考上面的總文章數\n",
    "                indices = np.argsort(weight[n])[::-1]  # transformer = TfidfTransformer()  #X.toarray()[5] 是第幾篇新聞的意思\n",
    "                # 看TOP多少的詞\n",
    "                top_n = 40\n",
    "                top_features.append([features[i] for i in indices[:top_n]])    #這邊的寫法會讓關鍵字中間會有空白     \n",
    "\n",
    "            obj=collection.find(\n",
    "                 {\"$and\":[                   \n",
    "                        {\"comp\":{\"$regex\":\"udn\"}},\n",
    "                        {\"date\":{:\"20160{}{}\".format(aa,ii)}}           #計算當月文章數\n",
    "                        ]},{\"_id\":0})\n",
    "            for i in range(0,a):  #迴圈參考上面的總文章數\n",
    "                collection.update({\"_id\":obj[i][\"_id\"]},{\"$set\":{\"tfidf\":top_features[i]}}) #collection.update即可\n",
    "\n",
    "            tEnd = time.time() #記時(時間結束)\n",
    "            print '完成',len(top_features),'筆'    \n",
    "\n",
    "            del content[:]  #清除list暫存記憶體\n",
    "            del top_features[:]  #清除list暫存記憶體\n",
    "\n",
    "            print ' 花了 {} s'.format(tEnd-tStart)    #將經過時間print 出來"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
